Members: Jyothi Shankar and Singaram Subramanyan

In the first task, we replaced missing ID values with NA (using awk -F’,’ '{ if ($1 == "") $1 = "NA"; print $0 }' OFS=‘,’ AB_NYC_2019.csv > AB_NYC_2019_1.csv) and removed rows with missing values (using awk -F, '{ for (i = 1; i <= NF; i++) if ($i == "") next; print $0 }' AB_NYC_2019_1.csv > AB_NYC_2019.csv). We removed replaced missing ID values with NA because the ID isn't as crucial to the analysis therefore, replacing the ID with NA would still allow us to presverve columns that may only have the ID missing. We removed other rows with missing values because columns such as neighborhood are crucial to the analysis and missing values in such columns can skew the analysis. 

In the second task, we removed duplicate values (using sort -t’,’ -k1 AB_NYC_2019.csv | uniq > AB_NYC_2019_1.csv). We removed duplicate values because duplicate values can disrupt the dataset, skewing the analysis derived from the dataset.

In the third task, we removed outliers in the minimum nights column by first calculating the mean (using mean_value=$(awk -F',' '{sum += $11; count++} END {print sum/count}' AB_NYC_2019_1.csv) and then filtering out the dataset using the mean (using awk -F',' -v mean="$mean_value" '{ if ($11 < mean * 1.75 && $11 > mean * 0.25) print $0 }' OFS=',' AB_NYC_2019_1.csv > AB_NYC_2019.csv). We choose this method because this allowed us to keep the values so that their not too low or too high. We choose 25% of the mean as the lower bound because this would be low enough to not exclude reasonable data but not too low that unreasonable data is included. We choose 175% of the mean because some listings may only be long term listings, so the minimum night value had to be high enough that these listing were also considered, but it could not be too high that unreasonable data is also included.  
